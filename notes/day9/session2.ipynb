{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3d398e",
   "metadata": {},
   "source": [
    "# Distance Metrics in KNN \n",
    "KNN relies on “distance” to find the closest neighbors. Different metrics work best for different data shapes and scales.\n",
    "\n",
    "## Common Distance Metrics\n",
    "- **Euclidean Distance**: The straight-line distance between two points in Euclidean space.\n",
    "- **Manhattan Distance**: The sum of the absolute differences of their coordinates.\n",
    "- **Minkowski Distance**: A generalization of both Euclidean and Manhattan distances.\n",
    "- **Hamming Distance**: The number of positions at which the corresponding elements are different (used for categorical variables).\n",
    "- **Cosine Similarity**: Measures the cosine of the angle between two non-zero vectors (used for text data).\n",
    "- **Mahalanobis Distance**: Accounts for correlations between variables and is scale-invariant.\n",
    "- **Chebyshev Distance**: The maximum absolute difference along any coordinate dimension.\n",
    "\n",
    "\n",
    "# Choosing K value \n",
    "\n",
    "## 1. Use cross-validation (best approach)\n",
    "Try different values of K, e.g. 1 to 25, and choose the one with the highest validation accuracy. \n",
    "\n",
    "## 2. Rule of thumb\n",
    "A common heuristic is to set K to the square root of the number of data points (N). For example, if you have 100 data points, K would be 10.\n",
    "\n",
    "## 3. K should be odd for 2-class classification\n",
    "\n",
    "Avoids ties (equal votes).\n",
    "\n",
    "## 4. Scale your features before selecting K\n",
    "\n",
    "KNN is distance-based → must use:\n",
    "\n",
    "- Standardization\n",
    "\n",
    "- Normalization\n",
    "\n",
    "Otherwise one feature may dominate.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
