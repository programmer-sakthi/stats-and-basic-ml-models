{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94dda711",
   "metadata": {},
   "source": [
    "// ...existing code...\n",
    "# Model building with multiple predictors\n",
    "\n",
    "Model Building with Multiple Predictors in Multiple Linear Regression (MLR)\n",
    "\n",
    "Multiple Linear Regression (MLR) predicts a continuous outcome using two or more predictors. The model quantifies each predictor's contribution while holding others constant.\n",
    "\n",
    "## 1. Model equation\n",
    "\n",
    "For predictors X1, X2, ..., Xk:\n",
    "\n",
    "Y = β0 + β1 X1 + β2 X2 + ... + βk Xk + ϵ\n",
    "\n",
    "- Y: dependent variable  \n",
    "- β0: intercept  \n",
    "- βi: slope coefficients (effect of Xi on Y, holding other predictors constant)  \n",
    "- ϵ: error term\n",
    "\n",
    "## 2. Steps to build an MLR model\n",
    "\n",
    "1. Define the objective (e.g., predict house price, salary).  \n",
    "2. Select potential predictors: numerical, categorical, derived features.  \n",
    "3. Exploratory Data Analysis (EDA): distributions, outliers, correlations, missing values, scaling.  \n",
    "4. Encode categorical variables: one-hot for nominal, ordinal encoding for ordered categories.  \n",
    "5. Fit the model.\n",
    "\n",
    "Example (Python):\n",
    "```python\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "X = df[['age','income','education_level']]\n",
    "X = sm.add_constant(X)\n",
    "y = df['salary']\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "## 3. Model evaluation\n",
    "\n",
    "- R²: proportion of variance explained by predictors  \n",
    "- Adjusted R²: R² penalized for unnecessary predictors  \n",
    "- p-values: test significance of individual predictors  \n",
    "- F-statistic: overall model significance  \n",
    "- Residual diagnostics: linearity, homoscedasticity, normality, outliers, influential points\n",
    "\n",
    "## 4. Handling multiple predictors\n",
    "\n",
    "- Feature selection: forward selection, backward elimination, stepwise selection  \n",
    "- Regularization: LASSO, Ridge, Elastic Net\n",
    "\n",
    "## 5. Multicollinearity\n",
    "\n",
    "Highly correlated predictors can destabilize coefficients. Use:\n",
    "- Correlation heatmap  \n",
    "- Variance Inflation Factor (VIF)\n",
    "\n",
    "Example:\n",
    "```python\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "# VIF > 10 indicates serious multicollinearity\n",
    "```\n",
    "\n",
    "## 6. Model refinement\n",
    "\n",
    "- Remove insignificant or highly correlated predictors  \n",
    "- Add polynomial or interaction terms if needed\n",
    "\n",
    "## 7. Final interpretation\n",
    "\n",
    "Each coefficient explains how much Y changes when a predictor changes, holding other variables constant.  \n",
    "Example: β2 = 3.5 for income → “Salary increases by 3.5 units for every 1-unit increase in income, holding other variables constant.”\n",
    "// ...existing code..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
